<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Weiliang Tang's Homepage</title>
  
  <meta name="author" content="Meirui Jiang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/Meirui_Cartoon.JPG">
</head>

<body>
  <table style="width:100%;max-width:1200px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/weiliang_tang.JPG", target="_blank"><img style="width:100%;max-width:100%" alt="profile photo" src="images/weiliang_tang.JPG" class="hoverZoomLink"></a>
            </td>
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:left; display: flex; align-items: center;" >
                <name style="font-size: 48px;">Weiliang Tang </name>
              </p>
              <p style="font-size: 24px; color: red;">(Website Under Construction)</p>
              <p style="line-height: 1.8em;">Ph.D Student
                <br>
                Department of ​Computer Science and Engineering
                <br>
                The Chinese University of Hong Kong
                <br>
                Office: Room 906, Ho Sin-Hang Engineering Building
                <br>
                Email: williamtang1024 [at] cse.cuhk.edu.hk
              </p>
              
              <!-- <p style="text-align:left;">
                <span class="icon" style="margin: 0.8%;"><a href="https://scholar.google.com/citations?user=TuyGlzwAAAAJ&hl=en", target="_blank"><img src="images/icons/scholar_bbg.png" alt="google scholar"></a></span>
                <span class="icon" style="margin: 0.8%;"><a href="https://github.com/MeiruiJiang", target="_blank"><img src="images/icons/github_2.png" alt="gitjub"></a></span>
                <span class="icon" style="margin: 0.8%;"><a href="https://www.linkedin.com/in/meirui-jiang-b9a545186/", target="_blank"><img src="images/icons/linkedin2.png" alt="linkedin"></a></span>
              </p> -->
            </td>

          </tr>
        </tbody></table>

        <heading>Short Bio</heading>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding-left:20px;width:100%;vertical-align:middle">
              <p style="color:red">
              <b>
              I'm currently on the job market. Please reach out if you think I would be a good fit. Thank you!
              </b>
              </p>
              <p>
                I am a final year Ph.D student in <a href="http://www.cse.cuhk.edu.hk/", target="_blank">Department of Computer Science and Engineering (CSE)</a>, <a href="https://www.cuhk.edu.hk/english/", target="_blank">The Chinese University of Hong Kong (CUHK)</a>, supervised by <a href="http://www.cse.cuhk.edu.hk/~cwfu/", target="_blank">Prof. Chi-Wing Fu</a>. Previously, I received the B. Eng. degree from <a href="https://cuhk.edu.hk/", target="_blank">The Chinese University of Hong Kong (CUHK)</a> in 2021.
              </p>
              <p>
                <!-- My research interest lies in AI (with explainability, privacy and generalizability) for healthcare, aiming to promote the applicability of AI research. Recently I focus on federated learning. -->
                <!-- My research interest lies in machine learning with applications for medical image analysis. I aim to promote the applicability (regarding robustness, effiency, generalizability, reliability and privacy) of AI research for healthcare. Recently I focus on federated learning. -->
                My previous research focuses on Embodiment AI, which achieve robot manipulation given only language description and is generalizable to any tasks, any robots, any objects, and any scenarios. The final goal is to build something like the "dummy" or the "JARVIS" in the movie Iron Man. Before this, I have rich experience in 3D computer vision, autonomous driving, and few-shot learning.
              </p>
            </td>
          </tr>
        </tbody></table>

        <!-- <heading>News</heading>
        <table class="news" width="100%" align="center" border="0" cellpadding="0"><tbody>
          <tr>
            <td width="100%" valign="center">
              <nav>
              <ul style="height: auto; overflow:hidden; overflow-y:scroll; height:250px; width:100%;">
                <li> [2024-10] One paper on communication-efficient federated learning is accepted by NeurIPS 2024.</li>
              </ul>
            </nav>
            </td>
          </tr>
        </tbody></table> -->
    
        
        <heading>Publications</heading>
        <table class="paper-container"><tbody>
          <br><br>
          <em style="padding-left: 3%;">* denotes the co-first author and <sup>&dagger;</sup> denotes the corresponding author.</em>
            <!-- Paper item -->
            <tr>
              <td class="paper-item-img">
                <img src="images/papers/geoconstraint_teaser.png" alt="clean-usnob" width="250" height="">
              </td>
              <td width="55%" valign="middle">
                <p style="line-height: 1.8em;">
                <papertitle style="font-size: 22px;">Geometric Constraints as General Interface for Robotic Manipulations</papertitle>
                <br>
                <strong>Weiliang Tang</strong>, Jiahui Pan, Yun-Hui Liu, Masayoshi Tomizuka, Li Erran Li, Chi-Wing Fu<sup>&dagger;</sup>, Mingyu Ding
                <br>
                <em> To be submited to The International Conference on Machine Learning, (<b>ICML</b>), 2025.</em>
                <br>
                [<a href="",  target="_blank">paper</a>] [<a href="",  target="_blank">code</a>] 
                <p><video controls width="70% " playsinline=" " autoplay=" " loop=" " preload=" " muted="true">
                    <source src="videos/geoconst_publication_video.mp4" type="video/mp4">
                </video></p>
              </p>
              </td>
            </tr> 
            <!-- Paper item -->
            <tr>
                <td class="paper-item-img">
                  <img src="images/papers/embodi-agnostic.png" alt="clean-usnob" width="500" height="">
                </td>
                <td width="55%" valign="middle">
                  <p style="line-height: 1.8em;">
                  <papertitle style="font-size: 22px;">Embodiment-Agnostic Action Planning via Object-Part Scene Flow
                  </papertitle>
                  <br>
                  <strong>Weiliang Tang</strong>, Weiliang Tang, Jia-Hui Pan, Wei Zhan, Jianshu Zhou, Huaxiu Yao, Yun-Hui Liu, Masayoshi Tomizuka, Mingyu Ding<sup>&dagger;</sup>, and Chi-Wing Fu<sup>&dagger;</sup>
                  <br>
                  <em>Submited to International Conference on Robotics and Automation, (<b>ICRA</b>), 2025.</em>
                  <br>
                  [<a href="https://www.arxiv.org/pdf/2409.10032",  target="_blank">paper</a>] [<a href="",  target="_blank">code</a>] 
                  <p><video controls width="70% " playsinline=" " autoplay=" " loop=" " preload=" " muted="true">
                      <source src="videos/embodi-agnostic_publication_video.mp4" type="video/mp4">
                  </video></p>
                </p>
                </td>
            </tr> 
            <!-- Paper item -->
            <tr>
                <td class="paper-item-img">
                  <img src="images/papers/TNNLS.png" alt="clean-usnob" width="500" height="">
                </td>
                <td width="55%" valign="middle">
                  <p style="line-height: 1.8em;">
                  <papertitle style="font-size: 22px;">Overcoming Support Dilution for Robust Few-shot Semantic Segmentation
                  </papertitle>
                  <br>
                  <strong>Weiliang Tang*</strong>, Biqi Yang*, Pheng-Ann Heng, Yun-Hui Liu, Chi-Wing Fu<sup>&dagger;</sup>
                  <br>
                  <em>Submited to IEEE Transactions on Neural Networks and Learning Systems, (<b>TNNLS</b>), 2025.</em>
                  <br>
                  [<a href="",  target="_blank">paper</a>] [<a href="",  target="_blank">code</a>] 
                  <p><img src="images/papers/TNNLS_exp.png" alt="clean-usnob" width="500" height=""></p>
                </p>
                </td>
            </tr> 
            <!-- Paper item -->
            <tr>
                <td class="paper-item-img">
                  <img src="images/papers/nips.png" alt="clean-usnob" width="500" height="">
                </td>
                <td width="55%" valign="middle">
                  <p style="line-height: 1.8em;">
                    <papertitle style="font-size: 22px;">Prototypical Variational Autoencoder for Few-shot 3D Point Cloud Object Detection
                    </papertitle>
                  <br>
                  <strong>Weiliang Tang*</strong>, Biqi Yang*, Xianzhi Li, Pheng-Ann Heng, Yun-Hui Liu, Chi-Wing Fu<sup>&dagger;</sup>
                  <br>
                  <em>Accepted by Conference on Neural Information Processing Systems, (<b>NeurIPS</b>), 2025.</em>
                  <br>
                  [<a href="https://openreview.net/pdf?id=fljrZsJ2I8",  target="_blank">paper</a>] [<a href="",  target="_blank">code</a>] 
                  <p><img src="images/papers/nips_exp.png" alt="clean-usnob" width="500" height=""></p>
                </p>
                </td>
            </tr> 
            <!-- Paper item -->
            <tr>
                <td class="paper-item-img">
                  <img src="images/papers/arxiv.png" alt="clean-usnob" width="500" height="">
                </td>
                <td width="55%" valign="middle">
                  <p style="line-height: 1.8em;">
                    <papertitle style="font-size: 22px;">SKU-Patch: Towards Efficient Instance Segmentation for Unseen Objects in Auto-Store
                    </papertitle>
                  <br>
                  Biqi Yang*, <strong>Weiliang Tang*</strong>, Xiaojie Gao, Xianzhi Li, Yun-Hui Liu, Chi-Wing Fu, Pheng-Ann Heng<sup>&dagger;</sup>
                  <br>
                  <em>Submited to Arxiv, 2023.</em>
                  <br>
                  [<a href="https://arxiv.org/pdf/2012.03015",  target="_blank">paper</a>] [<a href="",  target="_blank">code</a>] 
                  <p><img src="images/papers/arxiv_exp.png" alt="clean-usnob" width="500" height=""></p>
                </p>
                </td>
            </tr> 
            <!-- Paper item -->
            <tr>
                <td class="paper-item-img">
                  <img src="images/papers/AAAI.png" alt="clean-usnob" width="250" height="">
                </td>
                <td width="55%" valign="middle">
                  <p style="line-height: 1.8em;">
                    <papertitle style="font-size: 22px;">SKU-Patch: Towards Efficient Instance Segmentation for Unseen Objects in Auto-Store
                    </papertitle>
                  <br>
                  Wu Zheng <strong>Weiliang Tang</strong>, Sijin Chen, Li Jiang, Chi-Wing Fu<sup>&dagger;</sup>
                  <br>
                  <em>Accepted by The Association for the Advancement of Artificial Intelligence (AAAI), 2021.</em>
                  <br>
                  [<a href="https://arxiv.org/pdf/2311.04645",  target="_blank">paper</a>] [<a href="https://github.com/Vegeta2020/SE-SSD",  target="_blank">code</a>] 
                  <p><img src="images/papers/AAAI_exp.png" alt="clean-usnob" width="500" height=""></p>
                </p>
                </td>
            </tr> 
            <!-- Paper item -->
            <tr>
                <td class="paper-item-img">
                  <img src="images/papers/cvpr.png" alt="clean-usnob" width="250" height="">
                </td>
                <td width="55%" valign="middle">
                  <p style="line-height: 1.8em;">
                    <papertitle style="font-size: 22px;">SE-SSD: Self-Ensembling Single-Stage Object Detector From Point Cloud
                    </papertitle>
                  <br>
                  Wu Zheng <strong>Weiliang Tang</strong>, Jiang Li, Chi-Wing Fu<sup>&dagger;</sup>
                  <br>
                  <em>Accepted by Conference on Computer Vision and Pattern Recognition (CVPR), 2021.</em>
                  <br>
                  [<a href="https://arxiv.org/pdf/2104.09804",  target="_blank">paper</a>] [<a href="https://github.com/Vegeta2020/SE-SSD",  target="_blank">code</a>] 
                  <p><img src="images/papers/cvpr_exp.png" alt="clean-usnob" width="500" height=""></p>
                </p>
                </td>
            </tr> 
        <br>
        



        <table class="news" width="100%" align="center" border="0" cellpadding="0"><tbody>
          <tr>
            <td width="100%" valign="center">
              <subheading>Teaching Experience</subheading>
              <ul style="height: auto;">
                <li>2021-2022 &ensp;Fall &ensp;&emsp;&nbsp;​Introduction to Algorithm(CSCI 3160).</li>
                <li>2021-2022 &ensp;Spring &ensp;Principle of Software Engineering (CSCI 3180).</li>
              </ul>
            </td>
          </tr>
        </tbody></table>
        
        <footer>
          <p style="text-align:center"> © Weiliang Tang | Last updated: Oct 2024</p>
          <p style="text-align:center;font-size:small;">
            Design and source code from <a style="font-size:small;" href="https://meriuijiang.github.io", target="_blank">Meirui Jiang's website</a>.
          </a></p>
        </footer>


      </td>
    </tr>
  </table>
</body>

</html>